{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation, digits\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "#==============================================================================\n",
    "#===  PART I  =================================================================\n",
    "#==============================================================================\n",
    "\n",
    "def get_order(n_samples):\n",
    "    try:\n",
    "        with open(str(n_samples) + '.txt') as fp:\n",
    "            line = fp.readline()\n",
    "            return list(map(int, line.split(',')))\n",
    "    except FileNotFoundError:\n",
    "        random.seed(1)\n",
    "        indices = list(range(n_samples))\n",
    "        random.shuffle(indices)\n",
    "        return indices\n",
    "    \n",
    "\n",
    "\n",
    "def hinge_loss_single(feature_vector, label, theta, theta_0):\n",
    "    \"\"\"\n",
    "    Finds the hinge loss on a single data point given specific classification\n",
    "    parameters.\n",
    "\n",
    "    Args:\n",
    "        `feature_vector` - numpy array describing the given data point.\n",
    "        `label` - float, the correct classification of the data\n",
    "            point.\n",
    "        `theta` - numpy array describing the linear classifier.\n",
    "        `theta_0` - float representing the offset parameter.\n",
    "    Returns:\n",
    "        the hinge loss, as a float, associated with the given data point and\n",
    "        parameters.\n",
    "    \"\"\"\n",
    "    margin = label * (np.dot(theta, feature_vector) + theta_0)\n",
    "\n",
    "    return max(0, 1 - margin)\n",
    "\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "\n",
    "def hinge_loss_full(feature_matrix, labels, theta, theta_0):\n",
    "    \"\"\"\n",
    "    Finds the hinge loss for given classification parameters averaged over a\n",
    "    given dataset\n",
    "\n",
    "    Args:\n",
    "        `feature_matrix` - numpy matrix describing the given data. Each row\n",
    "            represents a single data point.\n",
    "        `labels` - numpy array where the kth element of the array is the\n",
    "            correct classification of the kth row of the feature matrix.\n",
    "        `theta` - numpy array describing the linear classifier.\n",
    "        `theta_0` - real valued number representing the offset parameter.\n",
    "    Returns:\n",
    "        the hinge loss, as a float, associated with the given dataset and\n",
    "        parameters.  This number should be the average hinge loss across all of\n",
    "    \"\"\"\n",
    "\n",
    "    # Your code here\n",
    "    n = len(labels)\n",
    "    loss = 0\n",
    "    for i in range(n):\n",
    "        loss += hinge_loss_single(feature_matrix[i], labels[i], theta, theta_0)\n",
    "    return loss / n\n",
    "\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "\n",
    "def perceptron_single_step_update(\n",
    "        feature_vector,\n",
    "        label,\n",
    "        current_theta,\n",
    "        current_theta_0):\n",
    "    \"\"\"\n",
    "    Updates the classification parameters `theta` and `theta_0` via a single\n",
    "    step of the perceptron algorithm.  Returns new parameters rather than\n",
    "    modifying in-place.\n",
    "\n",
    "    Args:\n",
    "        feature_vector - A numpy array describing a single data point.\n",
    "        label - The correct classification of the feature vector.\n",
    "        current_theta - The current theta being used by the perceptron\n",
    "            algorithm before this update.\n",
    "        current_theta_0 - The current theta_0 being used by the perceptron\n",
    "            algorithm before this update.\n",
    "    Returns a tuple containing two values:\n",
    "        the updated feature-coefficient parameter `theta` as a numpy array\n",
    "        the updated offset parameter `theta_0` as a floating point number\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    theta = current_theta\n",
    "    theta_0 = current_theta_0\n",
    "    margin = label * (np.dot(theta, feature_vector) + theta_0)\n",
    "    if margin <= 0:\n",
    "        theta += label * feature_vector\n",
    "        theta_0 += label\n",
    "    return theta, theta_0\n",
    "\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "\n",
    "def perceptron(feature_matrix, labels, T):\n",
    "    \"\"\"\n",
    "    Runs the full perceptron algorithm on a given set of data. Runs T\n",
    "    iterations through the data set: we do not stop early.\n",
    "\n",
    "    NOTE: Please use the previously implemented functions when applicable.\n",
    "    Do not copy paste code from previous parts.\n",
    "\n",
    "    Args:\n",
    "        `feature_matrix` - numpy matrix describing the given data. Each row\n",
    "            represents a single data point.\n",
    "        `labels` - numpy array where the kth element of the array is the\n",
    "            correct classification of the kth row of the feature matrix.\n",
    "        `T` - integer indicating how many times the perceptron algorithm\n",
    "            should iterate through the feature matrix.\n",
    "\n",
    "    Returns a tuple containing two values:\n",
    "        the feature-coefficient parameter `theta` as a numpy array\n",
    "            (found after T iterations through the feature matrix)\n",
    "        the offset parameter `theta_0` as a floating point number\n",
    "            (found also after T iterations through the feature matrix).\n",
    "    \n",
    "    NOTE: Please call get_order(feature_matrix.shape[0]), \n",
    "        and use the ordering to iterate the feature matrix in each iteration. \n",
    "        The ordering is specified due to grading purpose. \n",
    "        In practice, people typically just randomly shuffle indices \n",
    "        to do stochastic optimization.\n",
    "    \"\"\"\n",
    "\n",
    "    # Your code here\n",
    "    n = len(labels)\n",
    "    theta = np.zeros(len(feature_matrix[0]))\n",
    "    theta_0 = 0\n",
    "    \n",
    "    for t in range(T):\n",
    "        for i in get_order(n):\n",
    "            theta, theta_0 = perceptron_single_step_update(feature_matrix[i], labels[i], theta, theta_0)\n",
    "\n",
    "    return theta, theta_0\n",
    "\n",
    "\n",
    "\n",
    "def average_perceptron(feature_matrix, labels, T):\n",
    "    \"\"\"\n",
    "    Runs the average perceptron algorithm on a given dataset.  Runs `T`\n",
    "    iterations through the dataset (we do not stop early) and therefore\n",
    "    averages over `T` many parameter values.\n",
    "\n",
    "    NOTE: Please use the previously implemented functions when applicable.\n",
    "    Do not copy paste code from previous parts.\n",
    "\n",
    "    NOTE: It is more difficult to keep a running average than to sum and\n",
    "    divide.\n",
    "\n",
    "    Args:\n",
    "        `feature_matrix` -  A numpy matrix describing the given data. Each row\n",
    "            represents a single data point.\n",
    "        `labels` - A numpy array where the kth element of the array is the\n",
    "            correct classification of the kth row of the feature matrix.\n",
    "        `T` - An integer indicating how many times the perceptron algorithm\n",
    "            should iterate through the feature matrix.\n",
    "\n",
    "    Returns a tuple containing two values:\n",
    "        the average feature-coefficient parameter `theta` as a numpy array\n",
    "            (averaged over T iterations through the feature matrix)\n",
    "        the average offset parameter `theta_0` as a floating point number\n",
    "            (averaged also over T iterations through the feature matrix).\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    n = len(labels)\n",
    "    theta = np.zeros(len(feature_matrix[0]))\n",
    "    theta_0 = 0\n",
    "    theta_sum = np.zeros(len(feature_matrix[0]))\n",
    "    theta_0_sum = 0\n",
    "\n",
    "    for t in range(T):\n",
    "        for i in get_order(n):\n",
    "            theta, theta_0 = perceptron_single_step_update(feature_matrix[i], labels[i], theta, theta_0)\n",
    "            theta_sum += theta\n",
    "            theta_0_sum += theta_0\n",
    "\n",
    "    return theta_sum / (n * T), theta_0_sum / (n * T)\n",
    "\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "\n",
    "def pegasos_single_step_update(\n",
    "        feature_vector,\n",
    "        label,\n",
    "        L,\n",
    "        eta,\n",
    "        theta,\n",
    "        theta_0):\n",
    "    \"\"\"\n",
    "    Updates the classification parameters `theta` and `theta_0` via a single\n",
    "    step of the Pegasos algorithm.  Returns new parameters rather than\n",
    "    modifying in-place.\n",
    "\n",
    "    Args:\n",
    "        `feature_vector` - A numpy array describing a single data point.\n",
    "        `label` - The correct classification of the feature vector.\n",
    "        `L` - The lamba value being used to update the parameters.\n",
    "        `eta` - Learning rate to update parameters.\n",
    "        `theta` - The old theta being used by the Pegasos\n",
    "            algorithm before this update.\n",
    "        `theta_0` - The old theta_0 being used by the\n",
    "            Pegasos algorithm before this update.\n",
    "    Returns:\n",
    "        a tuple where the first element is a numpy array with the value of\n",
    "        theta after the old update has completed and the second element is a\n",
    "        real valued number with the value of theta_0 after the old updated has\n",
    "        completed.\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    margin = label * (np.dot(theta, feature_vector) + theta_0)\n",
    "    if margin <= 1:\n",
    "        theta = (1 - eta * L) * theta + eta * label * feature_vector\n",
    "        theta_0 += eta * label\n",
    "    else:\n",
    "        theta = (1 - eta * L) * theta\n",
    "\n",
    "    return theta, theta_0\n",
    "\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "\n",
    "def pegasos(feature_matrix, labels, T, L):\n",
    "    \"\"\"\n",
    "    Runs the Pegasos algorithm on a given set of data. Runs T iterations\n",
    "    through the data set, there is no need to worry about stopping early.  For\n",
    "    each update, set learning rate = 1/sqrt(t), where t is a counter for the\n",
    "    number of updates performed so far (between 1 and nT inclusive).\n",
    "\n",
    "    NOTE: Please use the previously implemented functions when applicable.  Do\n",
    "    not copy paste code from previous parts.\n",
    "\n",
    "    Args:\n",
    "        `feature_matrix` - A numpy matrix describing the given data. Each row\n",
    "            represents a single data point.\n",
    "        `labels` - A numpy array where the kth element of the array is the\n",
    "            correct classification of the kth row of the feature matrix.\n",
    "        `T` - An integer indicating how many times the algorithm\n",
    "            should iterate through the feature matrix.\n",
    "        `L` - The lamba value being used to update the Pegasos\n",
    "            algorithm parameters.\n",
    "\n",
    "    Returns:\n",
    "        a tuple where the first element is a numpy array with the value of the\n",
    "        theta, the linear classification parameter, found after T iterations\n",
    "        through the feature matrix and the second element is a real number with\n",
    "        the value of the theta_0, the offset classification parameter, found\n",
    "        after T iterations through the feature matrix.\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    n = len(labels)\n",
    "    theta = np.zeros(len(feature_matrix[0]))\n",
    "    theta_0 = 0\n",
    "    count = 0\n",
    "\n",
    "    for t in range(T):\n",
    "        for i in get_order(n):\n",
    "            count += 1\n",
    "            eta = 1 / np.sqrt(count)\n",
    "            theta, theta_0 = pegasos_single_step_update(feature_matrix[i], labels[i], L, eta, theta, theta_0)\n",
    "\n",
    "    return theta, theta_0\n",
    "\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "\n",
    "#==============================================================================\n",
    "#===  PART II  ================================================================\n",
    "#==============================================================================\n",
    "\n",
    "\n",
    "\n",
    "##  #pragma: coderesponse template\n",
    "##  def decision_function(feature_vector, theta, theta_0):\n",
    "##      return np.dot(theta, feature_vector) + theta_0\n",
    "##  def classify_vector(feature_vector, theta, theta_0):\n",
    "##      return 2*np.heaviside(decision_function(feature_vector, theta, theta_0), 0)-1\n",
    "##  #pragma: coderesponse end\n",
    "\n",
    "\n",
    "\n",
    "def classify(feature_matrix, theta, theta_0):\n",
    "    \"\"\"\n",
    "    A classification function that uses given parameters to classify a set of\n",
    "    data points.\n",
    "\n",
    "    Args:\n",
    "        `feature_matrix` - numpy matrix describing the given data. Each row\n",
    "            represents a single data point.\n",
    "        `theta` - numpy array describing the linear classifier.\n",
    "        `theta_0` - real valued number representing the offset parameter.\n",
    "\n",
    "    Returns:\n",
    "        a numpy array of 1s and -1s where the kth element of the array is the\n",
    "        predicted classification of the kth row of the feature matrix using the\n",
    "        given theta and theta_0. If a prediction is GREATER THAN zero, it\n",
    "        should be considered a positive classification.\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    return 2 * np.heaviside(np.dot(feature_matrix, theta) + theta_0, 0) - 1\n",
    "\n",
    "\n",
    "\n",
    "def classifier_accuracy(\n",
    "        classifier,\n",
    "        train_feature_matrix,\n",
    "        val_feature_matrix,\n",
    "        train_labels,\n",
    "        val_labels,\n",
    "        **kwargs):\n",
    "    \"\"\"\n",
    "    Trains a linear classifier and computes accuracy.  The classifier is\n",
    "    trained on the train data.  The classifier's accuracy on the train and\n",
    "    validation data is then returned.\n",
    "\n",
    "    Args:\n",
    "        `classifier` - A learning function that takes arguments\n",
    "            (feature matrix, labels, **kwargs) and returns (theta, theta_0)\n",
    "        `train_feature_matrix` - A numpy matrix describing the training\n",
    "            data. Each row represents a single data point.\n",
    "        `val_feature_matrix` - A numpy matrix describing the validation\n",
    "            data. Each row represents a single data point.\n",
    "        `train_labels` - A numpy array where the kth element of the array\n",
    "            is the correct classification of the kth row of the training\n",
    "            feature matrix.\n",
    "        `val_labels` - A numpy array where the kth element of the array\n",
    "            is the correct classification of the kth row of the validation\n",
    "            feature matrix.\n",
    "        `kwargs` - Additional named arguments to pass to the classifier\n",
    "            (e.g. T or L)\n",
    "\n",
    "    Returns:\n",
    "        a tuple in which the first element is the (scalar) accuracy of the\n",
    "        trained classifier on the training data and the second element is the\n",
    "        accuracy of the trained classifier on the validation data.\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    theta, theta_0 = classifier(train_feature_matrix, train_labels, **kwargs)\n",
    "    train_predictions = classify(train_feature_matrix, theta, theta_0)\n",
    "    val_predictions = classify(val_feature_matrix, theta, theta_0)\n",
    "    train_accuracy = np.mean(train_predictions == train_labels)\n",
    "    val_accuracy = np.mean(val_predictions == val_labels)\n",
    "    return train_accuracy, val_accuracy\n",
    "\n",
    "\n",
    "\n",
    "def extract_words(text):\n",
    "    \"\"\"\n",
    "    Helper function for `bag_of_words(...)`.\n",
    "    Args:\n",
    "        a string `text`.\n",
    "    Returns:\n",
    "        a list of lowercased words in the string, where punctuation and digits\n",
    "        count as their own words.\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    for c in punctuation + digits:\n",
    "        text = text.replace(c, ' ' + c + ' ')\n",
    "    return text.lower().split()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def bag_of_words(texts, remove_stopword=False):\n",
    "    \"\"\"\n",
    "    NOTE: feel free to change this code as guided by Section 3 (e.g. remove\n",
    "    stopwords, add bigrams etc.)\n",
    "\n",
    "    Args:\n",
    "        `texts` - a list of natural language strings.\n",
    "    Returns:\n",
    "        a dictionary that maps each word appearing in `texts` to a unique\n",
    "        integer `index`.\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    stopwords = set()\n",
    "    if remove_stopword:\n",
    "        stopwords = set([\"a\", \"an\", \"the\", \"and\", \"or\", \"but\", \"if\", \"while\", \"with\", \"to\", \"of\", \"in\", \"on\", \"for\", \"at\", \"by\", \"from\", \"up\", \"down\", \"out\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"])\n",
    "          \n",
    "    indices_by_word = {}  # maps word to unique index\n",
    "    for text in texts:\n",
    "        word_list = extract_words(text)\n",
    "        for word in word_list:\n",
    "            if remove_stopword and word in stopwords:\n",
    "                continue\n",
    "            if word not in indices_by_word:\n",
    "                indices_by_word[word] = len(indices_by_word)\n",
    "\n",
    "    return indices_by_word\n",
    "\n",
    "\n",
    "\n",
    "def extract_bow_feature_vectors(reviews, indices_by_word, binarize=True):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        `reviews` - a list of natural language strings\n",
    "        `indices_by_word` - a dictionary of uniquely-indexed words.\n",
    "    Returns:\n",
    "        a matrix representing each review via bag-of-words features.  This\n",
    "        matrix thus has shape (n, m), where n counts reviews and m counts words\n",
    "        in the dictionary.\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    feature_matrix = np.zeros([len(reviews), len(indices_by_word)], dtype=np.float64)\n",
    "    for i, text in enumerate(reviews):\n",
    "        word_list = extract_words(text)\n",
    "        for word in word_list:\n",
    "            if word not in indices_by_word: continue\n",
    "            feature_matrix[i, indices_by_word[word]] += 1\n",
    "    if binarize:\n",
    "        # Your code here\n",
    "        feature_matrix = (feature_matrix > 0).astype(float)\n",
    "    return feature_matrix\n",
    "\n",
    "\n",
    "\n",
    "def accuracy(preds, targets):\n",
    "    \"\"\"\n",
    "    Given length-N vectors containing predicted and target labels,\n",
    "    returns the fraction of predictions that are correct.\n",
    "    \"\"\"\n",
    "    return (preds == targets).mean()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mitcourse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
